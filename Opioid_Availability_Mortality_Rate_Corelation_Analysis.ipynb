{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umakanetkar/opioid-mortality-corelation-analysis/blob/main/Opioid_Availability_Mortality_Rate_Corelation_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and Setup"
      ],
      "metadata": {
        "id": "QD8SQS5o24oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to see if theres a strong corelation between opioid availabilty and mortality rates in the state of Pennsylvania. There has been a marked rise in drug overdose related deaths and combined with an increase in drug availability, it makes a compelling case to find out this correlation.\n",
        "\n",
        "To come up with data based conclusions, I am doing analysis on large publically available opioid datasets.\n"
      ],
      "metadata": {
        "id": "eqtHXLwG3E8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "KmQHzkqXqAMU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install libkrb5-dev\n",
        "!wget https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "! pip install pyspark --user\n",
        "! pip install seaborn --user\n",
        "! pip install plotly --user\n",
        "! pip install imageio --user\n",
        "! pip install folium --user\n",
        "! pip3 install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gZFmXAZ1qJEH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3BqB1KwyqJAT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('Opioid-Project').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Xc57g0TsqI8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32b78f5-e91f-43b6-e0d5-34c6187677c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.7.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "nRkcvdrJqI5k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "#misc\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "\n",
        "#graph section\n",
        "import networkx as nx\n",
        "#import heapq  # for getting top n number of things from list,dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "\n",
        "# Time conversions\n",
        "import time\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = '/content/spark-2.4.5-bin-hadoop2.7'\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "H-BL5f5FqI2I"
      },
      "outputs": [],
      "source": [
        "#spark session\n",
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opioid Data"
      ],
      "metadata": {
        "id": "z7s48j4f7JY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I am using data sourced from the [Washington Post](https://www.washingtonpost.com/investigations/interactive/2023/opioid-epidemic-pain-pills-sold-oxycodone-hydrocodone/), in their publication of the DEA's opioid data. I have then selected the PA data from the Washington Post database.\n",
        "\n",
        "I am chosing to go with this dataset because it was quite recently published (2019) is quite reliable, and contains the features needed for regression analysis. Additionally, data collection is consistent, and metadata is freely available and open-source."
      ],
      "metadata": {
        "id": "yZ9_zPvO7OAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlnhTvkhqIzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5bb038-1e93-407a-e661-f742fefffcaf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-28 01:30:30--  https://www.washingtonpost.com/wp-stat/dea-pain-pill-database/summary/arcos-pa-statewide-itemized.csv.gz\n",
            "Resolving www.washingtonpost.com (www.washingtonpost.com)... 23.208.39.209\n",
            "Connecting to www.washingtonpost.com (www.washingtonpost.com)|23.208.39.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308798971 (294M) [text/csv]\n",
            "Saving to: ‘arcos-pa-statewide-itemized.csv.gz.1’\n",
            "\n",
            "arcos-pa-statewide- 100%[===================>] 294.49M  34.5MB/s    in 9.9s    \n",
            "\n",
            "2024-05-28 01:30:50 (29.8 MB/s) - ‘arcos-pa-statewide-itemized.csv.gz.1’ saved [308798971/308798971]\n",
            "\n",
            "gzip: arcos-pa-statewide-itemized.csv already exists; do you wish to overwrite (y or n)? "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!wget https://www.washingtonpost.com/wp-stat/dea-pain-pill-database/summary/arcos-pa-statewide-itemized.csv.gz\n",
        "!gunzip -k arcos-pa-statewide-itemized.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the unzipped dataset to spark and taking a look at first few rows to get a general sense of data."
      ],
      "metadata": {
        "id": "XbWlckab7_Jw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN-lYHvaqIv4"
      },
      "outputs": [],
      "source": [
        "opioid_sdf = spark.read.csv('arcos-pa-statewide-itemized.csv', header=True)\n",
        "\n",
        "opioid_sdf.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a temp view to query this table."
      ],
      "metadata": {
        "id": "P-dErNMK8ZMp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JC6DuKXqIsx"
      },
      "outputs": [],
      "source": [
        "opioid_sdf.createOrReplaceTempView(\"opioid_tbl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From an exploratory data analysis perspective, I am interested to look at the market shares of different quantities (in particular, quantity of units purchased, total MME, and number of transactions) for distributors from year to year.\n",
        "\n",
        "To do so, we aggregate our data in Spark by grouping by distributor name and transaction year with the following SQL query. This gives the total quantity, MME, and number of transactions for each company from year to year, so it must be divided by the overall yearly total to get the market share; i.e.\n",
        "market share\n",
        "t\n",
        "=\n",
        "distributor total\n",
        "t\n",
        "overall total\n",
        "t\n",
        ", where\n",
        "t\n",
        " is the year.\n",
        "\n",
        "Once the SQL query is complete, I will convert the resulting table to a Pandas dataframe."
      ],
      "metadata": {
        "id": "XYQIoILH8xr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KulOyBsJqIpc"
      },
      "outputs": [],
      "source": [
        "eda_query = '''SELECT BUYER_NAME AS DISTRIBUTOR,\n",
        "               RIGHT(TRANSACTION_DATE, 4) AS TRANS_YEAR,\n",
        "               SUM(QUANTITY) AS YEAR_QTY,\n",
        "               SUM(MME) AS YEAR_MME,\n",
        "               COUNT(BUYER_NAME) AS YEAR_TRANS\n",
        "               FROM opioid_tbl\n",
        "               GROUP BY BUYER_NAME, TRANS_YEAR'''\n",
        "\n",
        "eda_df = spark.sql(eda_query).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlKl74foqImD"
      },
      "outputs": [],
      "source": [
        "print(str(eda_df.head()) + \"\\n\")\n",
        "print(str(eda_df.dtypes) + \"\\n\")\n",
        "print(eda_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6K7neTGBzm"
      },
      "source": [
        "Description of Columns:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "DISTRIBUTOR: The pharmacy that sells the opioids directly to consumers.\n",
        "\n",
        "TRANS_YEAR: The year during which all transactions in the data took place.\n",
        "\n",
        "YEAR_QTY: The total number of units sold, per the given year and distributor. Units are defined as the number of pills/tablets sold.\n",
        "\n",
        "YEAR_MME: The total sum of the MME metric, per the given year and distributor. The MME metric is defined as a universal morphine equivalence unit, which determines the strength of each transaction. This takes into account the type of unit, the number of units, the dosage, and type of opioid.\n",
        "\n",
        "YEAR_TRANS: The total number of individual transactions made, per the given year and distributor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZoyDu0rqIis"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrzVQCKGOhb"
      },
      "source": [
        "# Get Market Shares"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the yearly quantity, MME, and transaction totals for each distributor to their respective market shares, I am dividing these columns by the overall yearly quantity, MME, and transaction totals.\n",
        "\n"
      ],
      "metadata": {
        "id": "mTA81MytAr9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xl8npMLqIf0"
      },
      "outputs": [],
      "source": [
        "eda_df['QTY_MKT_SHARE'] = eda_df.YEAR_QTY\n",
        "eda_df['MME_MKT_SHARE'] = eda_df.YEAR_MME\n",
        "eda_df['TRANS_MKT_SHARE'] = eda_df.YEAR_TRANS\n",
        "\n",
        "\n",
        "def get_qty_mkt_ttl(year):\n",
        "  return(np.sum(eda_df.YEAR_QTY[eda_df.TRANS_YEAR==year]))\n",
        "\n",
        "def get_mme_mkt_ttl(year):\n",
        "  return(np.sum(eda_df.YEAR_MME[eda_df.TRANS_YEAR==year]))\n",
        "\n",
        "def get_trans_mkt_ttl(year):\n",
        "  return(np.sum(eda_df.YEAR_TRANS[eda_df.TRANS_YEAR==year]))\n",
        "\n",
        "years = pd.unique(eda_df['TRANS_YEAR'])\n",
        "\n",
        "for year in years:\n",
        "  eda_df.loc[eda_df['TRANS_YEAR']==year, 'QTY_MKT_SHARE'] = \\\n",
        "    eda_df.loc[eda_df['TRANS_YEAR']==year, 'QTY_MKT_SHARE'] / get_qty_mkt_ttl(year)\n",
        "  eda_df.loc[eda_df['TRANS_YEAR']==year, 'MME_MKT_SHARE'] = \\\n",
        "    eda_df.loc[eda_df['TRANS_YEAR']==year, 'MME_MKT_SHARE'] / get_mme_mkt_ttl(year)\n",
        "  eda_df.loc[eda_df['TRANS_YEAR']==year, 'TRANS_MKT_SHARE'] = \\\n",
        "    eda_df.loc[eda_df['TRANS_YEAR']==year, 'TRANS_MKT_SHARE'] / get_trans_mkt_ttl(year)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the first 5 rows and first 50 unique distributors."
      ],
      "metadata": {
        "id": "ytMjNUh2BEEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVUn8RA3qIZ2"
      },
      "outputs": [],
      "source": [
        "print(str(eda_df.head()) + \"\\n\")\n",
        "print(pd.unique(eda_df.DISTRIBUTOR)[1:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I see that we have a lot of instances where we are counting the same distributor multiple number of times. For eg. \"'GIANT PHARMACY #6291' 'GIANT PHARMACY #6087' 'GIANT PHARMACY #6112'\n",
        " 'GIANT PHARMACY #6045'\" all correspond to the same \"GIANT PHARMACY\"\n",
        "\n",
        " In further analysis, I will correct this."
      ],
      "metadata": {
        "id": "eSVvR-teBTnv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22pnr58KtH8"
      },
      "source": [
        "# Mortality Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving over to mortality data, I am sourcing it from [CDC Wonder](https://wonder.cdc.gov/cmf-icd10.html). For convenience, I uploaded this dataset to GitHub so that it could be uploaded directly to Colab from the internet."
      ],
      "metadata": {
        "id": "7Lp6bDDTDjPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/umakanetkar/opioid-mortality-corelation-analysis/blob/main/Compressed-Mortality-1999-2016.txt\n",
        "!wget https://raw.githubusercontent.com/prmrs2/gthub4455ghdrive/main/Compressed-Mortality-1999-2016.txt"
      ],
      "metadata": {
        "id": "mGPsBtYlDcZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data into a Pandas data frame."
      ],
      "metadata": {
        "id": "UCg1TtcVF1vB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BrdkZPkqIT2"
      },
      "outputs": [],
      "source": [
        "mortality_raw = pd.read_csv('Compressed-Mortality-1999-2016.txt', header=0, sep='\\t', lineterminator='\\r')\n",
        "\n",
        "mortality_raw.head(675)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdj2GeB4g58E"
      },
      "source": [
        "Right off the bat, we see the last few columns dont have actual data. We'll take a closer look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Ys_00BqIRH"
      },
      "outputs": [],
      "source": [
        "mortality_raw.loc[ 665: ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCLwXj_hGRO"
      },
      "source": [
        "As seen above, the rows at the end are just footnotes. This occurs from row 671 onward, so we exclude these rows from the data.\n",
        "\n",
        "Also worth noting that, each county has an associated row in which the Notes column has the value \"Total\" and the Year/Year Code columns have NaN values. These rows correspond to the cumulative number of deaths over all years for the county. Since I do not need this information, I will exclude these rows. Also dropping the Notes and County columns, as the information in the Notes column is qualitative and cannot be used for modeling and the information in the County column is encoded by County Code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSafNMtAqINo"
      },
      "outputs": [],
      "source": [
        "mortality = mortality_raw[0:670]\n",
        "mortality = mortality.loc[mortality.Notes != '\\n\"Total\"']\n",
        "mortality = mortality.drop(['Notes', 'County', \"Year Code\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udKyRsrYiV7n"
      },
      "source": [
        "Column Descriptions:\n",
        "\n",
        "County Code: The FIPS code of each county.\n",
        "\n",
        "Year: The year for the mortality rate for the county.\n",
        "\n",
        "Deaths: The total number of deaths in the given county and year.\n",
        "\n",
        "Population: The total population in the given county and year.\n",
        "\n",
        "Crude Rate: The mortality rate, given as the ratio of number of deaths per 100,000 people in the given county and year.\n",
        "\n",
        "Because the Crude Rate variable is the mortality rate per 100,000 citizens, I will convert this to the regular death rate (\n",
        "number of deaths\n",
        "population size\n",
        ") to get a value ranging from\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        " for later modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEbTXweEqIKv"
      },
      "outputs": [],
      "source": [
        "mortality.loc[:, 'Crude Rate'] = mortality.loc[:, 'Crude Rate']/100000\n",
        "\n",
        "mortality.head(603)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73lEKr8FitRA"
      },
      "source": [
        "# Zip Code/FIPS Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjfqFl8Cj38J"
      },
      "source": [
        "One issue is that the mortality data is broken down at the county-level, while the opioid transactions are grouped at the zip code level. In order to aggregate the opioid data to the county-level (uniquely identified by the five digit FIPS code), I obtained a data set from GitHub which relates FIPS codes to zip codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L1WNyJMqID9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Data4Democracy/zip-code-to-county/master/county-fips.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that data is uploaded into Colab, I will transfer it into Pandas."
      ],
      "metadata": {
        "id": "4gWHD77KKmwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_1BUlG1lBuZ"
      },
      "outputs": [],
      "source": [
        "data_type_dict = {'STATEFP': str, 'COUNTYFP': str, 'COUNTYNAME': str}\n",
        "\n",
        "zips_df = pd.read_csv('county-fips.csv', header=0, \\\n",
        "                      dtype=data_type_dict)[['STATEFP',  'COUNTYFP', \\\n",
        "                                             'COUNTYNAME', 'STATE']]\n",
        "\n",
        "zips_df['FIPS'] = zips_df.apply(lambda x: float(x[0] + x[1]), axis=1)\n",
        "zips_df['COUNTYNAME'] = zips_df['COUNTYNAME'].apply(lambda x: x.upper().split(' COUNTY')[0])\n",
        "zips_df = zips_df[ zips_df['STATE'] == 'PA']\n",
        "\n",
        "zips_df = zips_df.drop(['STATEFP', 'COUNTYFP', 'STATE'], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2vjNo3YoMrS"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output of the last opioid data cell, different stores from the same company (e.g. different Giant pharmacies) are treated as separate distributors. To remedy this, I am doing some basic text manipulation with regex.\n",
        "\n",
        "For example, the string 'GIANT PHARMACY #6054' would be converted to 'GIANT PHARMACY' and all trimmed strings are grouped together and columns are aggregated with a sum. So, any distributor that was originally of the form 'GIANT PHARMACY #XXXX' would have their totals and market shares summed together, which is valid as the columns in question are all additive."
      ],
      "metadata": {
        "id": "k2ojL2KkN0TE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmOB9y4TlBmg"
      },
      "outputs": [],
      "source": [
        "eda_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO6c5rojlBlW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_dist_sub(str):\n",
        "  return(re.split('( [0-9])|( #)', str)[0])\n",
        "\n",
        "eda_df.loc[:, 'DISTRIBUTOR'] =  eda_df.loc[:, 'DISTRIBUTOR'].apply(get_dist_sub)\n",
        "\n",
        "eda_df_grouped = eda_df.groupby(by = ['DISTRIBUTOR', 'TRANS_YEAR']).sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgH8vZ5_lBjH"
      },
      "outputs": [],
      "source": [
        "print(eda_df_grouped.head(30))\n",
        "# print(pd.unique(eda_df_grouped.DISTRIBUTOR)[1175:1225])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also assigning, each Distributor a unique index."
      ],
      "metadata": {
        "id": "34mxRQuwOTqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqhldL5IlBiB"
      },
      "outputs": [],
      "source": [
        "grouped_Dist_list = pd.unique(eda_df_grouped.DISTRIBUTOR)\n",
        "dict_of_grouped_Dist = {}\n",
        "\n",
        "for i in range(0, len(grouped_Dist_list)):\n",
        "  dict_of_grouped_Dist.update({grouped_Dist_list[i] : i})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogXwT2LTlBIx"
      },
      "outputs": [],
      "source": [
        "# This unique index is then added to eda_df for our later EDA.\n",
        "eda_df_grouped_Ind = eda_df_grouped.copy()\n",
        "eda_df_grouped_Ind['Dist_Ind'] = 0\n",
        "\n",
        "def get_Dist_Ind(dist):\n",
        "  return(dict_of_grouped_Dist.get(dist))\n",
        "\n",
        "eda_df_grouped_Ind['Dist_Ind'] = \\\n",
        "  eda_df_grouped_Ind['DISTRIBUTOR'].apply(lambda x: get_Dist_Ind(x))\n",
        "\n",
        "eda_df_grouped_Ind.head(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3mp-LdlqXsL"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When analyzing providers in Pennsylvania, it's important to consider the major players in the industry. However, using just one metric can be problematic. For instance, a provider might sell a large number of pills, but each pill might have a low opioid concentration. Additionally, some companies might have only recently entered the market. To address this, we need to examine annual trends in market share by quantity, transactions, and MME (morphine milligram equivalent), along with yearly mortality rates. By considering these factors, we can better understand any significant changes in market share."
      ],
      "metadata": {
        "id": "mygXnkw2OjsE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXjaZUP2qsym"
      },
      "source": [
        "### Quantity Market Share by Year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am beginning the EDA with producing a bar plot of the various market shares of annual quantity for different distributors each year.\n",
        "\n",
        "The most important thing to note is that each year there are only about 5-6 distributors with major proportion of the annual quantity market share. This suggests that in our later modeling, I will likely be able to focus on transactions from the top few companies without losing subtantial information.\n",
        "\n"
      ],
      "metadata": {
        "id": "021bXHPEQ41N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgFTih-rlBBN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "qty_bar_plot = sns.FacetGrid(eda_df_grouped_Ind, col=\"TRANS_YEAR\", \\\n",
        "                             col_wrap=3, sharex=True)\n",
        "qty_bar_plot.map(plt.plot, \"Dist_Ind\", \"QTY_MKT_SHARE\")\n",
        "qty_bar_plot.set_ylabels('Market Share of Quantity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPoY7qz5s3Fk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6z0oaF0rijG"
      },
      "outputs": [],
      "source": [
        "years = eda_df_grouped_Ind.TRANS_YEAR.unique().tolist()\n",
        "\n",
        "for year in years:\n",
        "  print(eda_df_grouped_Ind.loc[eda_df_grouped_Ind['TRANS_YEAR']==year, \\\n",
        "      ['DISTRIBUTOR','TRANS_YEAR','QTY_MKT_SHARE','Dist_Ind']].\\\n",
        "      sort_values(by=['QTY_MKT_SHARE'], ascending=False).head(10))\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyNEtIx5uyQy"
      },
      "source": [
        "### MME Market Share by Year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, we produce a similar bar plot of the market shares of annual MME for different distributors each year.\n",
        "\n",
        "We again observe that each year's chart is dominated by about 6-7 spikes, further providing credence to only using the top 5 or so distributors when it comes to ML model."
      ],
      "metadata": {
        "id": "OaL0s5_2RWY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyf5Qt7rsy3Y"
      },
      "outputs": [],
      "source": [
        "qty_bar_plot = sns.FacetGrid(eda_df_grouped_Ind, col=\"TRANS_YEAR\", \\\n",
        "                             col_wrap=3, sharex=True)\n",
        "qty_bar_plot.map(plt.plot, \"Dist_Ind\", \"MME_MKT_SHARE\")\n",
        "qty_bar_plot.set_ylabels('Market Share of MME')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVk-6itsu2bq"
      },
      "outputs": [],
      "source": [
        "## Get Top 10 Distributors for each year and their totals:\n",
        "\n",
        "years = pd.unique(eda_df_grouped_Ind['TRANS_YEAR'])\n",
        "\n",
        "for year in years:\n",
        "  print(eda_df_grouped_Ind.loc[eda_df_grouped_Ind['TRANS_YEAR']==year, ['DISTRIBUTOR','TRANS_YEAR','MME_MKT_SHARE','Dist_Ind']].\\\n",
        "             sort_values(by=['MME_MKT_SHARE'], ascending=False).head(10))\n",
        "  print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w1WcO0jvdSr"
      },
      "source": [
        "### Transactions Market Share by Year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also producing an analogous plot for yearly transaction market share.\n",
        "\n",
        "I see the similar observation holds: each year's plot has several spikes that make up the marjority of the annual transaction total.\n",
        "\n"
      ],
      "metadata": {
        "id": "sLzR4LvERyi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwBseU5EvZaz"
      },
      "outputs": [],
      "source": [
        "qty_bar_plot = sns.FacetGrid(eda_df_grouped_Ind, col=\"TRANS_YEAR\", \\\n",
        "                             col_wrap=3, sharex=True)\n",
        "qty_bar_plot.map(plt.plot, \"Dist_Ind\", \"TRANS_MKT_SHARE\")\n",
        "qty_bar_plot.set_ylabels('Market Share of Transactions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWExse-Ovg4U"
      },
      "outputs": [],
      "source": [
        "## Get Top 10 Distributors for each year and their totals:\n",
        "\n",
        "years = pd.unique(eda_df_grouped_Ind['TRANS_YEAR'])\n",
        "\n",
        "for year in years:\n",
        "  print(eda_df_grouped_Ind.loc[eda_df_grouped_Ind['TRANS_YEAR']==year, \\\n",
        "      ['DISTRIBUTOR','TRANS_YEAR','TRANS_MKT_SHARE','Dist_Ind']].\\\n",
        "      sort_values(by=['TRANS_MKT_SHARE'], ascending=False).head(10))\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyzPlAxawXgL"
      },
      "source": [
        "### Mortality Rate by Year"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, producing histogram visualizations to show the distribution of mortality rate per year across counties. We see that these visualizations show roughly normal distributions across years."
      ],
      "metadata": {
        "id": "VDVyE6hmSoiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ebjuOnrwRo-"
      },
      "outputs": [],
      "source": [
        "qty_bar_plot = sns.FacetGrid(mortality, col=\"Year\", col_wrap=3, sharex=True)\n",
        "qty_bar_plot.map(plt.hist, \"Crude Rate\", bins=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCpub3zRwfjU"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-querying the opioid data table, this time grouping by zipcode, distributor, and transaction year. Adding the grouping by zipcode so that we can build our model on units uniquely defined by (county, year) like the mortality data."
      ],
      "metadata": {
        "id": "EA_e1VJTS_od"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceOa7tkDwbN5"
      },
      "outputs": [],
      "source": [
        "opioid_query = '''SELECT\n",
        "                     BUYER_NAME AS DISTRIBUTOR,\n",
        "                     BUYER_COUNTY AS COUNTY_NAME,\n",
        "                     INT(RIGHT(TRANSACTION_DATE, 4)) AS TRANS_YEAR,\n",
        "                     SUM(QUANTITY) AS YEAR_QTY,\n",
        "                     SUM(MME) AS YEAR_MME,\n",
        "                     COUNT(BUYER_NAME) AS YEAR_TRANS\n",
        "                 FROM opioid_tbl\n",
        "                 GROUP BY DISTRIBUTOR, COUNTY_NAME, TRANS_YEAR'''\n",
        "\n",
        "raw_opioid_df = spark.sql(opioid_query).toPandas().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oeqYcKpw-Qm"
      },
      "outputs": [],
      "source": [
        "raw_opioid_df[['DISTRIBUTOR', 'COUNTY_NAME']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vGAJGhKxcNb"
      },
      "outputs": [],
      "source": [
        "#Getting distinct indexes for distributors, as done previously\n",
        "\n",
        "raw_opioid_df.loc[:, 'DISTRIBUTOR'] =  raw_opioid_df.loc[:, 'DISTRIBUTOR'].apply(get_dist_sub)\n",
        "\n",
        "opioid_df = raw_opioid_df.groupby(by = ['DISTRIBUTOR', 'TRANS_YEAR', 'COUNTY_NAME']).sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sifGSnazXFF"
      },
      "outputs": [],
      "source": [
        "print(str(opioid_df.head()) + \"\\n\")\n",
        "print(str(opioid_df.dtypes) + \"\\n\")\n",
        "print(str(opioid_df.shape) + \"\\n\")\n",
        "print(pd.unique(opioid_df.COUNTY_NAME))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvpL5b-4zdib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPP6Zq4Xzzaa"
      },
      "source": [
        "###  Calculating Market Shares\n",
        "Re-calculating the market shares for each distributor for a given year and county."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXznqfP6z3E0"
      },
      "outputs": [],
      "source": [
        "opioid_df['QTY_MKT_SHARE'] = opioid_df.YEAR_QTY\n",
        "opioid_df['MME_MKT_SHARE'] = opioid_df.YEAR_MME\n",
        "opioid_df['TRANS_MKT_SHARE'] = opioid_df.YEAR_TRANS\n",
        "\n",
        "\n",
        "def get_qty_mkt_ttl(year, county):\n",
        "  return(np.sum(opioid_df.loc[(opioid_df.TRANS_YEAR==year) & \\\n",
        "                              (opioid_df.COUNTY_NAME==county), 'YEAR_QTY']))\n",
        "\n",
        "def get_mme_mkt_ttl(year, county):\n",
        "  return(np.sum(opioid_df.loc[(opioid_df.TRANS_YEAR==year) & \\\n",
        "                              (opioid_df.COUNTY_NAME==county), 'YEAR_MME']))\n",
        "\n",
        "def get_trans_mkt_ttl(year, county):\n",
        "  return(np.sum(opioid_df.loc[(opioid_df.TRANS_YEAR==year) & \\\n",
        "                              (opioid_df.COUNTY_NAME==county), 'YEAR_TRANS']))\n",
        "\n",
        "years = pd.unique(opioid_df['TRANS_YEAR'])\n",
        "counties = pd.unique(opioid_df['COUNTY_NAME'])\n",
        "\n",
        "\n",
        "for year in years:\n",
        "  for county in counties:\n",
        "    opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                  (opioid_df['COUNTY_NAME']==county), \\\n",
        "                  'QTY_MKT_SHARE'] = \\\n",
        "      opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                    (opioid_df['COUNTY_NAME']==county), \\\n",
        "                    'QTY_MKT_SHARE'] / get_qty_mkt_ttl(year, county)\n",
        "\n",
        "    opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                  (opioid_df['COUNTY_NAME']==county), \\\n",
        "                  'MME_MKT_SHARE'] = \\\n",
        "      opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                    (opioid_df['COUNTY_NAME']==county), \\\n",
        "                    'MME_MKT_SHARE'] / get_mme_mkt_ttl(year, county)\n",
        "\n",
        "    opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                  (opioid_df['COUNTY_NAME']==county), \\\n",
        "                  'TRANS_MKT_SHARE'] = \\\n",
        "      opioid_df.loc[(opioid_df['TRANS_YEAR']==year) & \\\n",
        "                    (opioid_df['COUNTY_NAME']==county), \\\n",
        "                    'TRANS_MKT_SHARE'] / get_trans_mkt_ttl(year, county)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0If35oW0EqZ"
      },
      "outputs": [],
      "source": [
        "opioid_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWx1CU-2Rp-"
      },
      "source": [
        "### Join Opioid and FIPS Data\n",
        "\n",
        "Combining the opioid data with the FIPS dataset to aggregate to the county-level, in order to match up with our mortality data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOYhYjCu2GXW"
      },
      "outputs": [],
      "source": [
        "merged_opioid_df = opioid_df.merge(zips_df, \\\n",
        "                                   left_on='COUNTY_NAME', \\\n",
        "                                   right_on='COUNTYNAME').\\\n",
        "                                   drop_duplicates().\\\n",
        "                                   drop(['COUNTY_NAME', 'COUNTYNAME'], axis=1)\n",
        "\n",
        "print(str(merged_opioid_df.head(123892)) + \"\\n\")\n",
        "print(str(merged_opioid_df.shape) + \"\\n\")\n",
        "\n",
        "\n",
        "merged_opioid_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gul9t-gu27O1"
      },
      "source": [
        "### Aggregate to County-Level\n",
        "\n",
        "Aggregating data to the county-level, using FIPS in the groupby."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUxAzY2i2dZi"
      },
      "outputs": [],
      "source": [
        "final_opioid_df = merged_opioid_df.groupby(by = ['DISTRIBUTOR', 'TRANS_YEAR', 'FIPS']).sum().reset_index()\n",
        "\n",
        "final_opioid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl5LwzEo2_1Y"
      },
      "outputs": [],
      "source": [
        "final_opioid_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reformatting Opioid Data"
      ],
      "metadata": {
        "id": "NtSIYNb5uuyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that opioid data is prepped, I will reformate it so that it can be merged with the mortality data. This is because the mortality data rows are uniquely identified by (county, year), while the opioid data rows are uniquely identified by (county, year, distributor).\n",
        "\n",
        "I am selecting the top 5 distributors for each type of market share for every (county, year) pair. These are used as columns in a new data frame that's uniquely identified by (county, year) pairs. For any (county, year) pair that doesn't have 5 distributors,I will simply append NaN values."
      ],
      "metadata": {
        "id": "JERuvF4cUOZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_opioid_df = final_opioid_df.copy(deep=True)\n",
        "\n",
        "fips = pd.unique(new_opioid_df['FIPS'])\n",
        "years = pd.unique(new_opioid_df['TRANS_YEAR'])\n",
        "\n",
        "frames = []\n",
        "\n",
        "for fip in fips:\n",
        "  for year in years:\n",
        "    df_QTY = new_opioid_df.loc[(new_opioid_df['FIPS']==fip) & \\\n",
        "                               (new_opioid_df['TRANS_YEAR']==year), \\\n",
        "        ['TRANS_YEAR', 'FIPS', 'QTY_MKT_SHARE', 'MME_MKT_SHARE', 'TRANS_MKT_SHARE']].\\\n",
        "        sort_values(by=['QTY_MKT_SHARE'], ascending=False).head(5)\n",
        "    df_MME = new_opioid_df.loc[(new_opioid_df['FIPS']==fip) & \\\n",
        "                               (new_opioid_df['TRANS_YEAR']==year), \\\n",
        "        ['TRANS_YEAR', 'FIPS', 'QTY_MKT_SHARE', 'MME_MKT_SHARE', 'TRANS_MKT_SHARE']].\\\n",
        "        sort_values(by=['MME_MKT_SHARE'], ascending=False).head(5)\n",
        "    df_TRANS = new_opioid_df.loc[(new_opioid_df['FIPS']==fip) & \\\n",
        "                                 (new_opioid_df['TRANS_YEAR']==year), \\\n",
        "        ['TRANS_YEAR', 'FIPS', 'QTY_MKT_SHARE', 'MME_MKT_SHARE', 'TRANS_MKT_SHARE']].\\\n",
        "        sort_values(by=['TRANS_MKT_SHARE'], ascending=False).head(5)\n",
        "\n",
        "    vals = list()\n",
        "    vals.append(year)\n",
        "    vals.append(fip)\n",
        "\n",
        "    # If there are at least 5 distributors for this (FIPS, year) pair, take all\n",
        "    # of them\n",
        "    if(len(df_QTY) >= 5):\n",
        "      vals.append(df_QTY.iloc[0]['QTY_MKT_SHARE'])\n",
        "      vals.append(df_MME.iloc[0]['MME_MKT_SHARE'])\n",
        "      vals.append(df_TRANS.iloc[0]['TRANS_MKT_SHARE'])\n",
        "\n",
        "      vals.append(df_QTY.iloc[1]['QTY_MKT_SHARE'])\n",
        "      vals.append(df_MME.iloc[1]['MME_MKT_SHARE'])\n",
        "      vals.append(df_TRANS.iloc[1]['TRANS_MKT_SHARE'])\n",
        "\n",
        "      vals.append(df_QTY.iloc[2]['QTY_MKT_SHARE'])\n",
        "      vals.append(df_MME.iloc[2]['MME_MKT_SHARE'])\n",
        "      vals.append(df_TRANS.iloc[2]['TRANS_MKT_SHARE'])\n",
        "\n",
        "      vals.append(df_QTY.iloc[3]['QTY_MKT_SHARE'])\n",
        "      vals.append(df_MME.iloc[3]['MME_MKT_SHARE'])\n",
        "      vals.append(df_TRANS.iloc[3]['TRANS_MKT_SHARE'])\n",
        "\n",
        "      vals.append(df_QTY.iloc[4]['QTY_MKT_SHARE'])\n",
        "      vals.append(df_MME.iloc[4]['MME_MKT_SHARE'])\n",
        "      vals.append(df_TRANS.iloc[4]['TRANS_MKT_SHARE'])\n",
        "\n",
        "    # Otherwise, take as many as there are and append NaN for the rest\n",
        "    else:\n",
        "      ind = 0\n",
        "\n",
        "      for i in range(len(df_QTY)):\n",
        "        vals.append(df_QTY.iloc[i]['QTY_MKT_SHARE'])\n",
        "        vals.append(df_QTY.iloc[i]['MME_MKT_SHARE'])\n",
        "        vals.append(df_QTY.iloc[i]['TRANS_MKT_SHARE'])\n",
        "        ind = ind+1\n",
        "      while(ind < 5):\n",
        "        vals.append(np.nan)\n",
        "        vals.append(np.nan)\n",
        "        vals.append(np.nan)\n",
        "        ind = ind+1\n",
        "\n",
        "    df_per_year = pd.DataFrame([vals],\n",
        "        columns= ['TRANS_YEAR', 'FIPS', 'QTY_MKT_SHARE_1', 'MME_MKT_SHARE_1', 'TRANS_MKT_SHARE_1', \\\n",
        "                  'QTY_MKT_SHARE_2', 'MME_MKT_SHARE_2', 'TRANS_MKT_SHARE_2', \\\n",
        "                  'QTY_MKT_SHARE_3', 'MME_MKT_SHARE_3', 'TRANS_MKT_SHARE_3', \\\n",
        "                  'QTY_MKT_SHARE_4', 'MME_MKT_SHARE_4', 'TRANS_MKT_SHARE_4', \\\n",
        "                  'QTY_MKT_SHARE_5', 'MME_MKT_SHARE_5', 'TRANS_MKT_SHARE_5', \\\n",
        "                  ])\n",
        "    frames.append(df_per_year)\n",
        "\n",
        "result_df = pd.concat(frames).reset_index(drop=True)\n",
        "\n",
        "result_df #67 unique fips, 9 unique years = 603 rows\n"
      ],
      "metadata": {
        "id": "RNCRRU0Iusj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge Opioid and Mortality Data"
      ],
      "metadata": {
        "id": "r8P3uvpovBrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the opioid data is finally in correct format, I am merging it with the mortality data to get the final data frame for the modeling."
      ],
      "metadata": {
        "id": "oCw_-suXUzCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_df = mortality.merge(result_df, \\\n",
        "                           left_on=['County Code', 'Year'], \\\n",
        "                           right_on = ['FIPS', 'TRANS_YEAR']).\\\n",
        "                           drop(['County Code', 'Deaths', \\\n",
        "                                 'Population', 'TRANS_YEAR'], axis=1)\n",
        "\n",
        "model_df = model_df.loc[:, ['FIPS', 'Year', 'Crude Rate', \\\n",
        "                           'QTY_MKT_SHARE_1', 'QTY_MKT_SHARE_2', \\\n",
        "                           'QTY_MKT_SHARE_3', 'QTY_MKT_SHARE_4', \\\n",
        "                           'QTY_MKT_SHARE_5', \\\n",
        "                           'MME_MKT_SHARE_1', 'MME_MKT_SHARE_2', \\\n",
        "                           'MME_MKT_SHARE_3', 'MME_MKT_SHARE_4', \\\n",
        "                           'MME_MKT_SHARE_5', \\\n",
        "                           'TRANS_MKT_SHARE_1', 'TRANS_MKT_SHARE_2', \\\n",
        "                           'TRANS_MKT_SHARE_3', 'TRANS_MKT_SHARE_4', \\\n",
        "                           'TRANS_MKT_SHARE_5']]\n",
        "\n",
        "model_df"
      ],
      "metadata": {
        "id": "wWJc4Ek0vD_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping NaNs\n",
        "model_df_drop = model_df.dropna()\n",
        "model_df_drop"
      ],
      "metadata": {
        "id": "ZwN952IgvD8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the independent and dependent variables into separate objects."
      ],
      "metadata": {
        "id": "5ZesWDqbVEcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = model_df_drop['Crude Rate']\n",
        "features = model_df_drop.loc[:, model_df_drop.columns != 'Crude Rate']"
      ],
      "metadata": {
        "id": "VFU6IwZJvD3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjvtmWt-wV5c"
      },
      "source": [
        "### Test Train Split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now splitting the data using a 30/70 testing/training split for validation. Fixing the random seed for the split so that the analysis is consistent across notebook runs."
      ],
      "metadata": {
        "id": "Gcugb5ADVOKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "_eN6XoHvV9g9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWvOtUO2wZ7E"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "  train_test_split(features, label, test_size=0.3, random_state=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTcwZFUewa9V"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lm = LinearRegression()\n",
        "lm_model = lm.fit(x_train, y_train)\n",
        "lm_y_pred = lm.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saXmhPQ5wgO9"
      },
      "outputs": [],
      "source": [
        "print('MSE: ' + str(mean_squared_error(y_test, lm_y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItQc_fUPwjUF"
      },
      "outputs": [],
      "source": [
        "lm_score = lm.score(x_test, y_test)\n",
        "print('R^2: ' + str(lm_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although an\n",
        "R\n",
        "2\n",
        " value of ~0.19 may not seem particularly high, it is important to note that this means that just by looking at the market shares top 5 distributors for each (county, year) pair, we account for almost 20% of the variability in the annual mortality rate for each county. The strength of the association between these market shares and the mortality rate corroborates the notion that a leading cause of death in PA prior to the COVID-19 pandemic were deaths related opioid-usage.\n",
        "\n",
        "Now that we have looked at the overall model performance, we dig down to look at the specific model coefficient values."
      ],
      "metadata": {
        "id": "UizSson0WVW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av1pxM0awl87"
      },
      "outputs": [],
      "source": [
        "print(lm.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression\n",
        "\n",
        "Now trying a LASSO regression model to see if the regularization/shrinkage effect that this model has on the coefficients improves the performance.\n",
        "\n",
        "To determine the value of the\n",
        "α\n",
        " parameter, I am using 1000 rounds of 5 fold cross-validation with a fixed random seed for consistent runs of the notebook."
      ],
      "metadata": {
        "id": "r8E-BWQQzdgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "lasso = LassoCV(eps=0.0001, n_alphas=1000, cv=5, random_state=19)\n",
        "lasso_model = lasso.fit(x_train, y_train)\n",
        "lasso_y_pred = lasso.predict(x_test)"
      ],
      "metadata": {
        "id": "uYZjzXDZzOPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, checking the MSE and\n",
        "R\n",
        "2\n",
        " to compare performances."
      ],
      "metadata": {
        "id": "LxFn3HRaXG4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('MSE: ' + str(mean_squared_error(y_test, lasso_y_pred)))"
      ],
      "metadata": {
        "id": "L8sYrhL_znLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_score = lasso.score(x_test, y_test)\n",
        "print('R^2: ' + str(lasso_score))"
      ],
      "metadata": {
        "id": "mWm5VM_wzrVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V slight increase in MSE and decrease in R2 as compared to linear model.\n",
        "Checking the co-efficients to see if any coefficient was shrunk to 0."
      ],
      "metadata": {
        "id": "xiuv4wEjXX5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lasso_model.coef_)"
      ],
      "metadata": {
        "id": "EOB3rtpyzwEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression\n",
        "\n",
        "Now trying a ridge regression model which penalizes large coefficients, but not to the same extent as a LASSO regression.\n",
        "\n",
        "To determine the value of\n",
        "α\n",
        ",  using 1000 rounds of 5-fold cross validation."
      ],
      "metadata": {
        "id": "j7ECc4iZz-Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "ridge = RidgeCV(alphas=[n/10000 for n in range(1, 1000)], cv=5)\n",
        "ridge_model = ridge.fit(x_train, y_train)\n",
        "ridge_y_pred = ridge.predict(x_test)"
      ],
      "metadata": {
        "id": "mhdS5Ag2zyeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the model performance metrics to determine the effectiveness of the ridge regression."
      ],
      "metadata": {
        "id": "4uTjMPM_X-SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('MSE: ' + str(mean_squared_error(y_test, ridge_y_pred)))"
      ],
      "metadata": {
        "id": "MEXP2iKQ0EGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_score = ridge.score(x_test, y_test)\n",
        "print('R^2: ' + str(ridge_score))"
      ],
      "metadata": {
        "id": "uCh6gfen1x-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As compared to previous LASSO model, we see theres a slight uptick in R2 and decrease in MSE. This seems to suggest that indeed all of the chosen covariates can improve prediction of the mortality rates."
      ],
      "metadata": {
        "id": "gceEryAyYQen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Discussion\n"
      ],
      "metadata": {
        "id": "JmLlyo6p4eIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this project, I modeled annual county mortality rate as a function of the market shares of opioid transactions, quantity of opioid units sold to distributors, and MME of the top 5 major distributors in each county every year.\n",
        "\n",
        "Of all the models produced, the OLS (linear regression) seemed to perform the best, followed by the LASSO model, and finally the ridge model. The OLS model outperformed the other two models in terms of both MSE and\n",
        "R\n",
        "2\n",
        ". The fact that the other two models both implement some form of shrinkage of the covariate coefficients and performed worse indiciates that these variables are useful in explaining some of the variability in the annual county-level mortality in PA counties from 2006-2014.\n",
        "\n",
        "Although an\n",
        "R\n",
        "2\n",
        " of 0.19 may seem unassuming at a first glance, it is important to note that we are modeling all annual county-level mortality as a function of these opioid-related market shares. The fact that these covariates can explain almost 20% of this variability is further potential evidence of the fact that there is a strong relationship between opioid activity and mortality in PA, possibly even being an active cause of mortality.\n",
        "\n",
        "One of the biggest issues I faced during this project was getting various data sources together. For instance, the mortality data was identified by year and FIPS code, while the opioid data was identified by year and county name. Because of issues like this, I had to get external data sets to get columns to match up for our joins. It was also initially difficult finding data sources located online to make this notebook functional entirely in the cloud, but I got around this by using GitHub."
      ],
      "metadata": {
        "id": "Y1x-FheJ4jk3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7tkOi2o1x3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwOKELwx1xzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8M9MDWG1xwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0v_GsYQM1xtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-jsJp0U1xqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOCL440rJwua+C8efhpcd+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}